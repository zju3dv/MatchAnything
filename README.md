# MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training
### [Project Page](https://zju3dv.github.io/MatchAnything) | [Paper](https://arxiv.org/abs/2501.07556)

> MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training\
> [Xingyi He](https://hxy-123.github.io/),
[Hao Yu](https://ritianyu.github.io/),
[Sida Peng](https://pengsida.net),
[Dongli Tan](https://github.com/Cuistiano),
[Zehong Shen](https://zehongs.github.io),
[Hujun Bao](http://www.cad.zju.edu.cn/home/bao/)<sup>†</sup>,
[Xiaowei Zhou](https://xzhou.me/)<sup>†</sup> \
> Arxiv 2025

<p align="center">
    <img src=video/teaser_demo.gif alt="animated" />
</p>

## Quick Start

### [<img src="https://s2.loli.net/2024/09/15/aw3rElfQAsOkNCn.png" width="20"> HuggingFace demo for MatchAnything](https://huggingface.co/spaces/LittleFrog/MatchAnything)

## [Models](https://huggingface.co/spaces/LittleFrog/MatchAnything/blob/main/imcui/third_party/MatchAnything/README.md) are on HuggingFace! (Pre-trained weights are included) The training code will be available later.


## Citation

```
@inproceedings{he2025matchanything,
title={MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training},
author={He, Xingyi and Yu, Hao and Peng, Sida and Tan, Dongli and Shen, Zehong and Bao, Hujun and Zhou, Xiaowei},
booktitle={Arxiv},
year={2025}
}
```
